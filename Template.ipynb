{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mars Terrain Semantic Segmentation with Deep Learning: Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on semantic segmentation of 64x128 grayscale Mars terrain images, assigning each pixel to one of five terrain classes. Paired masks provide pixel-wise labels. The goal is to build a model for accurate classification, evaluated by mean intersection over union (mIoU) excluding the background class (label 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This template serves as a comprehensive tool for testing and running various models. To use it, create a `model.py` file and import it in the model section.\n",
    "\n",
    "The most notable models we tested are stored in the `models` folder, while the best version is executed in [`FinalModel.ipynb`](./FinalModel.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig_intro_](./images/template.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sJ2RL17G9r9"
   },
   "source": [
    "## Name your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1734111007222,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "puW40K10HZG1"
   },
   "outputs": [],
   "source": [
    "# Model name\n",
    "model_name = 'template'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1734111008860,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "p2M7G9V0hu-O"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Generate timestamp\n",
    "timestamp = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "# Unified filename for both the final model and checkpoint\n",
    "model_filename = f\"model_{model_name}_{timestamp}.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw_-hFm6bjY6"
   },
   "source": [
    "## Connect Colab to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24644,
     "status": "ok",
     "timestamp": 1734111033500,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "y2S4GWr3Uoa8",
    "outputId": "af9121a5-aad4-40a1-d32f-7a97f0406602"
   },
   "outputs": [],
   "source": [
    "COLAB = True\n",
    "\n",
    "if COLAB:\n",
    "    print(\"COLAB\")\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/gdrive\")\n",
    "    %cd /gdrive/My Drive\n",
    "else:\n",
    "    print(\"NO COLAB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTJ69O89_Whd"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8041,
     "status": "ok",
     "timestamp": 1734111041535,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "Ra3l1zBG_Whe",
    "outputId": "2c9a3bf0-f907-4967-cea1-3d3249ca8bad"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Numerical and data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Mixed precision setup\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# GPU configuration\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "# Sklearn for train-test splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# OpenCV for image processing\n",
    "import cv2\n",
    "\n",
    "# tqdm for progress visualization\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Print environment details\n",
    "print()\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {tfk.__version__}\")\n",
    "print(f\"GPU devices: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GN_cpHlSboXV"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cleaning process was applied to the training data to address the presence of outliers. Specifically, images with objectively incorrect masks were identified and removed, while for images with uncertain mask quality, most were retained to preserve diversity and encourage generalization.\n",
    "\n",
    "The final dataset used for training was saved as `clean_dataset.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4090,
     "status": "ok",
     "timestamp": 1734111045620,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "pLaoDaG1V1Yg",
    "outputId": "832d9f2a-5bb6-4a36-c500-4a92e20f1222"
   },
   "outputs": [],
   "source": [
    "# --------------- #\n",
    "# Load the dataset\n",
    "# --------------- #\n",
    "\n",
    "data_name = 'clean_dataset.npz'\n",
    "file_path = f\"data/{data_name}\"\n",
    "data = np.load(file_path)\n",
    "\n",
    "\n",
    "training_set = data[\"training_set\"]\n",
    "X_train = np.stack(training_set[:, 0], axis=0)  # Images\n",
    "y_train = np.stack(training_set[:, 1], axis=0)  # Masks\n",
    "\n",
    "X_test = data[\"test_set\"]  # Test Images\n",
    "\n",
    "# Preprocess images and masks\n",
    "if X_train.ndim == 3:\n",
    "    X_train = X_train[..., np.newaxis]\n",
    "if X_test.ndim == 3:\n",
    "    X_test = X_test[..., np.newaxis]\n",
    "\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "if y_train.ndim == 3:\n",
    "    y_train = y_train[..., np.newaxis]\n",
    "\n",
    "original_count = X_train.shape[0]\n",
    "print(f\"Number of images the {data_name} dataset: {original_count}\")\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    0: 'Background',\n",
    "    1: 'Soil',\n",
    "    2: 'Bedrock',\n",
    "    3: 'Sand',\n",
    "    4: 'Big Rock'\n",
    "}\n",
    "\n",
    "category_map = {key: key for key in label_mapping.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1734111045621,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "vztA7LBY_Whh",
    "outputId": "96420da5-3064-4763-bf89-b90544836714"
   },
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Print shapes of the datasets\n",
    "print(\"Shape of X_train_final:\", X_train_final.shape)\n",
    "print(\"Shape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_train_final:\", y_train_final.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the augmentation techniques we experimented with are detailed in the [`Augmentations.ipynb`](./Augmentations.ipynb) notebook.  \n",
    "\n",
    "In this template, we propose the augmentation strategy that was ultimately selected. To use it, import all the necessary functions from [`AugmentationsUtils.py`](./AugmentationsUtils.py). \n",
    "\n",
    "The `apply_augmentations` function applies the selected augmentations to the training set while retaining the original images. Additionally, it plots example outputs to visualize the effects of the augmentations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "executionInfo": {
     "elapsed": 3896,
     "status": "ok",
     "timestamp": 1734111049487,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "4uq6F4gy74c-",
    "outputId": "7965a896-a478-4ee9-d2f5-259e80b5c394"
   },
   "outputs": [],
   "source": [
    "from AugmentationsUtils import *\n",
    "\n",
    "# Augment the training set and plot images for the specified index\n",
    "augmented_X_train, augmented_y_train = apply_augmentations(\n",
    "    X_train_final,\n",
    "    y_train_final,\n",
    "    num_augmented= 3,  # Number of augmented versions per original image\n",
    "    plot_index= 0  # Index of the image to plot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xoXfIEt_Whi"
   },
   "source": [
    "##  Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please import your model from the `models` folder. This folder contains some of our best models, including the final model (nome.py). \n",
    "\n",
    "The final model ([`final_dual_branch.py`](./models/final_dual_branch.py)) is also imported and executed in the [`FinalModel.ipynb`](./FinalModel.ipynb) notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.you_name_model import *\n",
    "\n",
    "# Instantiate the model\n",
    "input_shape = augmented_X_train.shape[1:]  # Use the shape of the input images\n",
    "num_classes = 5  # As per dataset details\n",
    "model = get_model(input_shape=input_shape, num_classes=num_classes, dropout_rate=0, l2_reg=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions and metrics definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3287,
     "status": "ok",
     "timestamp": 1734111052766,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "CBkb3TRF1KJx"
   },
   "outputs": [],
   "source": [
    "# -------------------------------------- #\n",
    "#                  Metric                #\n",
    "# -------------------------------------- #\n",
    "\n",
    "class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n",
    "        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "        if labels_to_exclude is None:\n",
    "            labels_to_exclude = [0]  # Default to excluding label 0\n",
    "        self.labels_to_exclude = labels_to_exclude\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert predictions to class labels\n",
    "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "\n",
    "        # Flatten the tensors\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "        # Apply mask to exclude specified labels\n",
    "        mask = tf.reduce_all([tf.not_equal(y_true, label) for label in self.labels_to_exclude], axis=0)\n",
    "        y_true = tf.boolean_mask(y_true, mask)\n",
    "        y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "        # Update the state\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "\n",
    "# -------------------------------------- #\n",
    "#              Loss Functions            #\n",
    "# -------------------------------------- #\n",
    "\n",
    "\n",
    "def mean_iou_loss(num_classes, labels_to_exclude=None, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Mean IoU Loss Function.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of classes in the segmentation task.\n",
    "        labels_to_exclude: List of labels to exclude when calculating IoU.\n",
    "    Returns:\n",
    "        A function that calculates the Mean IoU loss.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # Convert predictions to class probabilities\n",
    "        y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "        # Convert true labels to one-hot encoding\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(tf.squeeze(y_true, axis=-1), tf.int32), num_classes)\n",
    "\n",
    "        # Flatten the tensors\n",
    "        y_true_flat = tf.reshape(y_true_one_hot, [-1, num_classes])\n",
    "        y_pred_flat = tf.reshape(y_pred, [-1, num_classes])\n",
    "\n",
    "        # Compute intersection and union\n",
    "        intersection = tf.reduce_sum(y_true_flat * y_pred_flat, axis=0)\n",
    "        union = tf.reduce_sum(y_true_flat + y_pred_flat, axis=0) - intersection\n",
    "\n",
    "        # Mask out excluded labels\n",
    "        if labels_to_exclude is not None:\n",
    "            for label in labels_to_exclude:\n",
    "                intersection = tf.tensor_scatter_nd_update(\n",
    "                    intersection,\n",
    "                    [[label]],\n",
    "                    [0.0]\n",
    "                )\n",
    "                union = tf.tensor_scatter_nd_update(\n",
    "                    union,\n",
    "                    [[label]],\n",
    "                    [0.0]\n",
    "                )\n",
    "\n",
    "        # Compute IoU for each class and average over classes\n",
    "        iou = (intersection + epsilon) / (union + epsilon)\n",
    "        mean_iou = tf.reduce_mean(iou)\n",
    "\n",
    "        # Invert to use as loss (lower IoU means higher loss)\n",
    "        return 1.0 - mean_iou\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def focal_loss_ignore_class_0(alpha=0.25, gamma=2.0, num_classes=None):\n",
    "    \"\"\"\n",
    "    Focal Loss Function that ignores class 0, with fixed alpha for all other classes.\n",
    "\n",
    "    Args:\n",
    "        alpha: Weighting factor for positive classes to address class imbalance.\n",
    "        gamma: Focusing parameter to down-weight easy examples.\n",
    "        num_classes: Number of classes in the segmentation task.\n",
    "    Returns:\n",
    "        A function that calculates the Focal Loss, ignoring class 0.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        # Convert predictions to class probabilities (softmax output)\n",
    "        y_pred = tf.nn.softmax(y_pred, axis=-1)\n",
    "\n",
    "        # Convert true labels to one-hot encoding\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(tf.squeeze(y_true, axis=-1), tf.int32), num_classes)\n",
    "\n",
    "        # Ignore class 0 in the loss calculation\n",
    "        y_true_no_bg = y_true_one_hot[..., 1:]  # Remove the first class (class 0)\n",
    "        y_pred_no_bg = y_pred[..., 1:]         # Remove the first class (class 0)\n",
    "\n",
    "        # Compute the focal loss\n",
    "        cross_entropy = -y_true_no_bg * tf.math.log(y_pred_no_bg + 1e-8)  # Cross entropy with stability\n",
    "\n",
    "        # Apply a fixed alpha for all classes and calculate the focal loss\n",
    "        weight = alpha * tf.math.pow(1 - y_pred_no_bg, gamma)  # Focusing weight\n",
    "        focal_loss = weight * cross_entropy\n",
    "\n",
    "        # Average over all pixels and classes (across the batch and spatial dimensions)\n",
    "        return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# -------------------------------------- #\n",
    "#                   Tools                #\n",
    "# -------------------------------------- #\n",
    "\n",
    "def apply_category_mapping(label):\n",
    "    \"\"\"\n",
    "    Apply category mapping to labels.\n",
    "    \"\"\"\n",
    "    keys_tensor = tf.constant(list(category_map.keys()), dtype=tf.int32)\n",
    "    vals_tensor = tf.constant(list(category_map.values()), dtype=tf.int32)\n",
    "\n",
    "    # Create the lookup table\n",
    "    table = tf.lookup.StaticHashTable(\n",
    "        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n",
    "        default_value=0\n",
    "    )\n",
    "\n",
    "    # Ensure the label is a Tensor\n",
    "    label = tf.convert_to_tensor(label, dtype=tf.int32)\n",
    "\n",
    "    return table.lookup(label)\n",
    "\n",
    "\n",
    "def create_segmentation_colormap(num_classes):\n",
    "    \"\"\"\n",
    "    Create a linear colormap using a predefined palette.\n",
    "    Uses 'viridis' as default because it is perceptually uniform\n",
    "    and works well for colorblindness.\n",
    "    \"\"\"\n",
    "    return plt.cm.viridis(np.linspace(0, 1, num_classes))\n",
    "\n",
    "\n",
    "def apply_colormap(label, colormap=None):\n",
    "    \"\"\"\n",
    "    Apply the colormap to a label.\n",
    "    \"\"\"\n",
    "    # Ensure label is 2D\n",
    "    label = np.squeeze(label)\n",
    "\n",
    "    if colormap is None:\n",
    "        num_classes = len(np.unique(label))\n",
    "        colormap = create_segmentation_colormap(num_classes)\n",
    "\n",
    "    # Apply the colormap\n",
    "    colored = colormap[label.astype(int)]\n",
    "\n",
    "    return colored\n",
    "\n",
    "class VizCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, images, labels, frequency=5):\n",
    "        \"\"\"\n",
    "        Initialize the visualization callback.\n",
    "\n",
    "        Args:\n",
    "            images: A batch of validation images (numpy array or tensor).\n",
    "            labels: Corresponding ground truth labels (numpy array or tensor).\n",
    "            frequency: Frequency (in epochs) to display visualizations.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.frequency == 0:  # Visualize only every \"frequency\" epochs\n",
    "            batch_size = len(self.images)\n",
    "            predictions = self.model.predict(self.images, verbose=0)\n",
    "            y_preds = tf.math.argmax(predictions, axis=-1).numpy()\n",
    "\n",
    "            # Define label mapping and custom colors\n",
    "            label_mapping = {\n",
    "                0: 'Background',\n",
    "                1: 'Soil',\n",
    "                2: 'Bedrock',\n",
    "                3: 'Sand',\n",
    "                4: 'Big Rock'\n",
    "            }\n",
    "            colors = [\n",
    "                '#000000',  # 0: Background \n",
    "                '#A95F44',  # 1: Soil \n",
    "                '#584C44',  # 2: Bedrock \n",
    "                '#C5986F',  # 3: Sand \n",
    "                '#A69980'   # 4: Big Rock \n",
    "            ]\n",
    "            cmap = ListedColormap(colors)\n",
    "\n",
    "            # Create legend patches\n",
    "            patches = [mpatches.Patch(color=colors[i], label=label_mapping[i]) for i in label_mapping]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                image = self.images[i]\n",
    "                label = self.labels[i]\n",
    "                label = apply_category_mapping(label)\n",
    "                y_pred = y_preds[i]\n",
    "\n",
    "                plt.figure(figsize=(18, 6))\n",
    "\n",
    "                # Input image\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.title(f'Input Image (Sample {i})')\n",
    "                if image.shape[-1] == 1:\n",
    "                    plt.imshow(image.squeeze(), cmap='gray')\n",
    "                else:\n",
    "                    plt.imshow(image)\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Ground truth mask\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.title('Ground Truth Mask')\n",
    "                plt.imshow(label.numpy().squeeze(), cmap=cmap, vmin=0, vmax=len(label_mapping) - 1)\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Predicted mask\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.title('Predicted Mask')\n",
    "                plt.imshow(y_pred.squeeze(), cmap=cmap, vmin=0, vmax=len(label_mapping) - 1)\n",
    "                plt.axis('off')\n",
    "\n",
    "                # Add legend\n",
    "                plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training settings\n",
    "Please substiutite your parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMAZER\n",
    "optimizer = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=1e-3,   # Starting learning rate\n",
    "    weight_decay=1e-4     # Regularization strength\n",
    ")\n",
    "\n",
    "# METRIC\n",
    "metric_main = 'sparse_categorical_accuracy'\n",
    "\n",
    "# LOSS FUNCTION\n",
    "loss_fn = # Choose a loss function and pass the required arguments\n",
    "\n",
    "# Compile the model with Dice Loss and accuracy metrics\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=[metric_main, MeanIntersectionOverUnion(num_classes=5, labels_to_exclude=[0])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and learning rate scheduler\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_mean_iou',   # Metric to monitor\n",
    "    patience=10,                # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True   # Restore the best model found\n",
    ")\n",
    "\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_mean_iou',    # Metric to monitor\n",
    "    factor=0.5,            # Factor by which the learning rate will be reduced\n",
    "    patience=5,            # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    verbose=1,             # Verbosity mode\n",
    "    min_lr=1e-6            # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "# Add ModelCheckpoint callback to save the best model based on validation loss\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_filename,  # Use the unified filename\n",
    "    monitor='val_mean_iou',  # Monitor validation metrics\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    mode='max',  # Mode to maximize the monitored quantity\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 493714,
     "status": "ok",
     "timestamp": 1734111546477,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "PtM0ubgdOzG-",
    "outputId": "b9b68d95-abe4-44b3-be33-fbcd564d635d"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 1000\n",
    "seed = 42\n",
    "\n",
    "viz_callback = VizCallback(X_val[:2], y_val[:2])  # Visualize first 2 validation images\n",
    "\n",
    "# Choose which callbacks to activate\n",
    "callbacks = [early_stopping, lr_scheduler, viz_callback, model_checkpoint]\n",
    "\n",
    "history = model.fit(\n",
    "    augmented_X_train, augmented_y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Retrieve best epoch and corresponding metrics from EarlyStopping based on Mean IoU\n",
    "best_epoch = history.history['val_mean_iou'].index(max(history.history['val_mean_iou']))  # Index of the best validation mIoU\n",
    "best_val_miou = max(history.history['val_mean_iou'])  # Best validation mIoU\n",
    "best_val_loss = history.history['val_loss'][best_epoch]  # Best validation loss at that epoch\n",
    "\n",
    "# Print the best metrics\n",
    "print(f\"Training completed after {best_epoch + 1} epochs (Early Stopping).\")\n",
    "print(f\"Best Validation Mean IoU: {best_val_miou:.4f}\")\n",
    "print(f\"Corresponding Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Save the final model after training\n",
    "model.save(model_filename)\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 50639,
     "status": "ok",
     "timestamp": 1734111597106,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "gRQn_vLs_Whl",
    "outputId": "339fe2ed-aaef-4322-eb89-cce1f4994be0"
   },
   "outputs": [],
   "source": [
    "# Load the most recent model\n",
    "model = tfk.models.load_model(model_filename, compile = False)\n",
    "print(f\"Model loaded from {model_filename}\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=[metric_main, MeanIntersectionOverUnion(num_classes=5, labels_to_exclude=[0])]\n",
    ")\n",
    "\n",
    "# Predictions\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "def y_to_df(y):\n",
    "    n_samples = len(y)\n",
    "    y_flat = y.reshape(n_samples, -1)\n",
    "    df = pd.DataFrame(y_flat)\n",
    "    df[\"id\"] = np.arange(n_samples)\n",
    "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
    "    return df[cols]\n",
    "\n",
    "# Save predictions to CSV\n",
    "submission_filename = f\"sub_{model_name}_{timestamp}.csv\"\n",
    "submission_df = y_to_df(preds)\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "# Download submission\n",
    "if COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(submission_filename)\n",
    "\n",
    "print(f\"Submission saved to {submission_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7712,
     "status": "ok",
     "timestamp": 1734111604814,
     "user": {
      "displayName": "maria aurora bertasini",
      "userId": "06547506204256234039"
     },
     "user_tz": -60
    },
    "id": "Zjzz5Xk18bdj",
    "outputId": "8f2a342d-4ce2-4c47-8691-46496a7ca6eb"
   },
   "outputs": [],
   "source": [
    "# Function to visualize predictions for a list of indices with a legend\n",
    "def visualize_predictions(idx_list):\n",
    "    # Ensure idx_list is a list even if a single integer is provided\n",
    "    if isinstance(idx_list, int):\n",
    "        idx_list = [idx_list]\n",
    "\n",
    "    # Create legend patches\n",
    "    patches = [mpatches.Patch(color=colors[i], label=label_mapping[i]) for i in label_mapping]\n",
    "\n",
    "    for idx in idx_list:\n",
    "        img = X_val[idx]\n",
    "        true_mask = y_val[idx]\n",
    "        pred_mask = model.predict(img[np.newaxis, ...])[0]\n",
    "        pred_mask = np.argmax(pred_mask, axis=-1)\n",
    "\n",
    "        plt.figure(figsize=(18, 6))\n",
    "\n",
    "        # Input Image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title(f'Input Image (Index: {idx})')\n",
    "        if img.shape[-1] == 1:\n",
    "            plt.imshow(img.squeeze(), cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # True Mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('True Mask')\n",
    "        plt.imshow(true_mask.squeeze(), cmap=cmap, vmin=0, vmax=len(label_mapping)-1)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predicted Mask\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.imshow(pred_mask.squeeze(), cmap=cmap, vmin=0, vmax=len(label_mapping)-1)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Add Legend\n",
    "        plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage with a list of indices\n",
    "visualize_predictions([i for i in range(27, 37)])  # Adjust the range as needed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
